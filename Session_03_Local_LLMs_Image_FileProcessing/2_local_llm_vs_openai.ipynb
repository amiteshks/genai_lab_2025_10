{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fb6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_question_local_llm(prompt):\n",
    "    # print(f\"User asked: {prompt}\")\n",
    "    # my_client.chat.completions.create\n",
    "\n",
    "    # Run a prompt against a local model (e.g., llama2)\n",
    "    response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assitant - Respond in one line\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d8e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "my_client = OpenAI(api_key=my_api_key)\n",
    "# my_client\n",
    "\n",
    "def ask_question_open_ai(prompt):\n",
    "\n",
    "    # print(f\"User asked: {prompt}\")\n",
    "    # my_client.chat.completions.create\n",
    "\n",
    "    llm_response = my_client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer as concisely as possible.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return llm_response.choices[0].message.content  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b41d830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from local LLM: The capital of France is Paris.\n",
      "Response from OpenAI: Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "response_local_llm = ask_question_local_llm(prompt)\n",
    "response_open_ai = ask_question_open_ai(prompt)\n",
    "\n",
    "print(f\"Response from local LLM: {response_local_llm}\")\n",
    "print(f\"Response from OpenAI: {response_open_ai}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7064ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------OPEN AI RESPONSE-------------------\n",
      "Time taken by OpenAI: 7.947659015655518 seconds\n",
      "\n",
      "OpenAI says: Best practice: treat API keys as secrets and store them in a dedicated secret management system, not in code or config files. Key steps:\n",
      "\n",
      "- Use a secrets manager (e.g., AWS Secrets Manager, Azure Key Vault, Google Secret Manager, HashiCorp Vault) or an external vault.\n",
      "- Never hard-code keys or commit them to VCS; avoid storing in config files checked into source control.\n",
      "- Restrict access with least privilege: per-service/per-environment keys, and fine-grained IAM roles or policies.\n",
      "- Fetch keys at runtime (or on deployment) rather than bundling them in images; if possible, inject via environment variables or the secret store API.\n",
      "- Rotate keys regularly; enable automatic rotation if available; revoke old keys promptly.\n",
      "- Encrypt secrets at rest and in transit; use your cloud KMS or built-in encryption features.\n",
      "- Enable auditing and alerts for secret access and usage anomalies.\n",
      "- For CI/CD: avoid exposing keys in logs; use ephemeral credentials or short-lived tokens; don’t persist them in build artifacts.\n",
      "- For containers/Kubernetes: use a secret store or encrypted Kubernetes Secrets; avoid embedding keys in manifests.\n",
      "- Have a recovery plan: rotate/revoke compromised keys quickly and document procedures.\n",
      "\n",
      "If you want, I can tailor these to your tech stack (cloud provider, Kubernetes, CI/CD, etc.).\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "Time taken by Local LLM: 2.2489750385284424 seconds\n",
      "\n",
      "Local LLM says: The best practice for storing API keys is to keep them securely in environment variables or a secrets management service, such as HashiCorp's Vault or AWS Secrets Manager, and avoid hardcoding them directly in your code.\n",
      "-------------------OPEN AI RESPONSE-------------------\n",
      "Time taken by OpenAI: 7.184406757354736 seconds\n",
      "\n",
      "OpenAI says: There isn’t a single “easiest” provider — it depends on your background and goals.\n",
      "\n",
      "- If you want a gentle, clean start: Google Cloud Platform (GCP) is often considered approachable.\n",
      "- If you’re in a Microsoft environment: Azure tends to be easiest due to familiar tooling (PowerShell, Visual Studio, Windows Server).\n",
      "- If you want market breadth and lots of tutorials: AWS is the most common choice, but it can be overwhelming at first.\n",
      "\n",
      "Quick-start tips:\n",
      "- Pick one provider and use its free tier.\n",
      "- Do a beginner path: AWS Cloud Practitioner, Azure Fundamentals, or GCP Associate Cloud Engineer.\n",
      "- Build a small project (e.g., VM, storage, basic network) to get hands-on practice.\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "Time taken by Local LLM: 1.7915911674499512 seconds\n",
      "\n",
      "Local LLM says: Based on general feedback, AWS (Amazon Web Services) is often considered the easiest cloud provider to learn for beginners, with its vast array of resources and tutorials available.\n",
      "-------------------OPEN AI RESPONSE-------------------\n",
      "Time taken by OpenAI: 6.414439916610718 seconds\n",
      "\n",
      "OpenAI says: - Property type: Industrial warehouse\n",
      "- Location: Fremont, CA\n",
      "- Building size: 80,000 sqft total\n",
      "- Available space: 60,000 sqft\n",
      "- Year built: 2008\n",
      "- Clear height: 24 ft\n",
      "- Loading docks: 6\n",
      "- Drive-ins: 2\n",
      "- Zoning: Light Industrial (M-1)\n",
      "- Systems/features: HVAC, sprinklers, modern security system\n",
      "- Parking: 50 spaces\n",
      "- Parking ratio: 2.5 per 1,000 sqft\n",
      "- Annual rent: $720,000\n",
      "- Rent per sqft per month: $0.75\n",
      "- Lease type: NNN\n",
      "- Cap rate: 6.5%\n",
      "- Contact: Contact for images\n",
      "\n",
      "Note: Parking details (50 spaces vs. 2.5/1,000 sqft) appear inconsistent with 60,000 sqft available.\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "Time taken by Local LLM: 7.626588821411133 seconds\n",
      "\n",
      "Local LLM says: Here are the extracted property details:\n",
      "\n",
      "* Location: Fremont, CA\n",
      "* Total building size: 80,000 sqft\n",
      "* Available space: 60,000 sqft\n",
      "* Year built: 2008\n",
      "* Features:\n",
      "\t+ 24-foot clear height\n",
      "\t+ 6 loading docks\n",
      "\t+ 2 drive-ins\n",
      "\t+ HVAC\n",
      "\t+ Sprinklers\n",
      "\t+ Modern security system\n",
      "* Zoning: Light industrial (M-1)\n",
      "* Parking: 50 spaces, 2.5:1,000 parking ratio\n",
      "* Rent: $720,000 per year ($0.75 per sqft/month), NNN lease\n",
      "* Cap rate: 6.5%\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    # Ask user for a question\n",
    "    user_prompt = input(\"Ask something: \")\n",
    "\n",
    "    if (user_prompt.lower() != 'quit'):\n",
    "        # Get and print the response\n",
    "        print (\"-------------------OPEN AI RESPONSE-------------------\")\n",
    "        start = time.time()\n",
    "        response_openai = ask_question_open_ai(user_prompt)\n",
    "        end = time.time()   \n",
    "        print(f\"Time taken by OpenAI: {end - start} seconds\")\n",
    "        print(\"\\nOpenAI says:\", response_openai)\n",
    "\n",
    "        print (\"\\n\\n-------------------LOCAL LLM RESPONSE-------------------\")\n",
    "        start = time.time()\n",
    "        response_local = ask_question_local_llm(user_prompt)\n",
    "        end = time.time()   \n",
    "        print(f\"Time taken by Local LLM: {end - start} seconds\")\n",
    "        print(\"\\nLocal LLM says:\", response_local)        \n",
    "\n",
    "        # add delay of 3 seconds\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        print(\"Exiting...\")\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a3762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
