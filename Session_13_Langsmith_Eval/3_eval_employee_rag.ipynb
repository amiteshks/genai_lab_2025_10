{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865d6195",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fd629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amiteshsinha/Training/10_2025_genai_lab/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 384)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Define Sample Documents\n",
    "documents = [\n",
    "    {\"section\": \"Employee Info\", \"content\": \"John's pay is processed on the 1st of every month.\"},\n",
    "    {\"section\": \"Employee Info\", \"content\": \"Mark is on a leave of absence until next Monday.\"},\n",
    "    {\"section\": \"Employee Info\", \"content\": \"Julie is a software engineer.\"},\n",
    "    {\"section\": \"Employee Info\", \"content\": \"Julie's pay is processed on the 1st of every month.\"},\n",
    "    {\"section\": \"Employee Info\", \"content\": \"Mark is a product manager.\"},\n",
    "    {\"section\": \"Employee Info\", \"content\": \"John is an AI architect and has salary of 500K USD.\"},\n",
    "]\n",
    "\n",
    "# Step 2: Get Content Texts\n",
    "content_corpus = [doc[\"content\"] for doc in documents]\n",
    "content_corpus\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "doc_vectors = model.encode(content_corpus)\n",
    "\n",
    "doc_vectors\n",
    "print(doc_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5c5714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"John's pay is processed on the 1st of every month.\",\n",
       "  'John is an AI architect and has salary of 500K USD.',\n",
       "  'Mark is a product manager.'],\n",
       " \"John's pay is processed on the 1st of every month.\\n---\\nJohn is an AI architect and has salary of 500K USD.\\n---\\nMark is a product manager.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: User Query and Semantic Matching\n",
    "import numpy as np\n",
    "\n",
    "query = \"Tell me about John's role.\"\n",
    "query_vec = model.encode([query])[0]\n",
    "# query_vec\n",
    "\n",
    "\n",
    "similarities = model.similarity(query_vec, doc_vectors)\n",
    "\n",
    "# Ensure it's a 1D numpy array\n",
    "similarities = np.asarray(similarities).squeeze()\n",
    "\n",
    "# Now get top 3\n",
    "top_3_indices = np.argsort(similarities)[::-1][:3]\n",
    "top_scores = similarities[top_3_indices]\n",
    "top_scores\n",
    "\n",
    "top_docs = [documents[i]['content'] for i in top_3_indices]\n",
    "\n",
    "top_docs\n",
    "context = \"\\n---\\n\".join(top_docs)\n",
    "top_docs, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed37ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "my_api_key = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "my_client = OpenAI(api_key=my_api_key)\n",
    "# my_client\n",
    "\n",
    "def ask_question_open_ai(prompt, context=''):\n",
    "    \"\"\"Call the LLM with the provided prompt and context.\n",
    "\n",
    "    IMPORTANT: use the passed-in prompt (not a global variable) so each\n",
    "    evaluation example can be answered correctly.\n",
    "    \"\"\"\n",
    "    llm_response = my_client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": '''\n",
    "             You are an assistant who answers only based on the given context.\n",
    "             '''},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nUser Question: {prompt}\"}\n",
    "        ]\n",
    "\n",
    "    )\n",
    "    return llm_response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b59025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about John's role.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Johnâ€™s role is AI architect.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (query)\n",
    "response = ask_question_open_ai(query, context)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944d889",
   "metadata": {},
   "source": [
    "#### Create a LangSmith Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be327d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset '2025Dec-Employee-Info-QA-Dataset-3' created with 4 examples.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"2025Dec-Employee-Info-QA-Dataset-3\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"When is John's pay processed?\", \"output\": \"John's pay is processed on the 1st of every month.\"},\n",
    "    {\"input\": \"What is Julie's job title?\", \"output\": \"Julie is a software engineer.\"},\n",
    "    {\"input\": \"What is John's salary?\", \"output\": \"John has a salary of 500K USD.\"},\n",
    "    {\"input\": \"What is Mark's current work status?\", \"output\": \"Mark is on a leave of absence until next Monday.\"},\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    client.create_example(inputs={\"question\": ex[\"input\"]}, outputs={\"answer\": ex[\"output\"]}, dataset_id=dataset.id)\n",
    "\n",
    "print(f\" Dataset '{dataset_name}' created with {len(examples)} examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985c735",
   "metadata": {},
   "source": [
    "##### Run LangSmith Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00fe32c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.42\n",
      "1.1.3\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --no-cache-dir langsmith\n",
    "\n",
    "import langsmith, langchain\n",
    "print(langsmith.__version__)\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95241bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip index versions langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70a5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from langsmith.evaluation import RunEvaluator\n",
    "\n",
    "class SimpleCorrectness(RunEvaluator):\n",
    "    \"\"\"LLM-as-a-judge correctness evaluator (version-safe).\"\"\"\n",
    "\n",
    "    def evaluate_run(self, run, example, **kwargs):\n",
    "        ref = example.outputs.get(\"answer\", \"\").strip()\n",
    "\n",
    "        pred = (\n",
    "            run.outputs.get(\"answer\")\n",
    "            or run.outputs.get(\"output_text\")\n",
    "            or run.outputs.get(\"result\")\n",
    "            or \"\"\n",
    "        ).strip()\n",
    "\n",
    "        question = example.inputs.get(\"question\", \"\")\n",
    "\n",
    "        if not ref or not pred:\n",
    "            return {\n",
    "                \"key\": \"correctness\",\n",
    "                \"score\": 0.0,\n",
    "                \"commentary\": \"Missing reference or prediction\",\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            score, reason = self.llm_as_a_judge(ref, pred, question)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"key\": \"correctness\",\n",
    "                \"score\": 0.0,\n",
    "                \"commentary\": f\"Evaluator error: {e}\",\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": score,\n",
    "            \"commentary\": reason,\n",
    "        }\n",
    "\n",
    "    def llm_as_a_judge(self, reference: str, prediction: str, question: str = \"\"):\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a semantic correctness evaluator.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "Question: {question}\n",
    "Reference answer: {reference}\n",
    "Model prediction: {prediction}\n",
    "\n",
    "Return JSON only:\n",
    "{{\"score\": <number between 0 and 1>, \"reason\": \"<short explanation>\"}}\n",
    "\"\"\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        data = json.loads(content)\n",
    "\n",
    "        score = float(data[\"score\"])\n",
    "        reason = data[\"reason\"]\n",
    "\n",
    "        return max(0.0, min(1.0, score)), reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d89eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable # Need to enable tracing on LangSmith\n",
    "\n",
    "# Define your target function that performs retrieval per-question\n",
    "@traceable \n",
    "def ask_question(inputs):\n",
    "    question = inputs[\"question\"]\n",
    "    # compute embedding for the question\n",
    "    query_vec = model.encode([question])[0]\n",
    "\n",
    "    # compute cosine similarities between query and doc_vectors\n",
    "    import numpy as np\n",
    "    q_norm = np.linalg.norm(query_vec) + 1e-8\n",
    "    doc_norms = np.linalg.norm(doc_vectors, axis=1) + 1e-8\n",
    "    sims = np.dot(doc_vectors, query_vec) / (doc_norms * q_norm)\n",
    "\n",
    "    # pick top-3 supporting docs and build context\n",
    "    top_3_indices = np.argsort(sims)[::-1][:3]\n",
    "    top_docs = [content_corpus[i] for i in top_3_indices]\n",
    "    context = \"\\n---\\n\".join(top_docs)\n",
    "\n",
    "    # call LLM with question and its retrieved context\n",
    "    answer = ask_question_open_ai(question, context)\n",
    "    return {\"answer\": answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65865a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from langsmith.evaluation import RunEvaluator\n",
    "\n",
    "class SimpleCorrectness(RunEvaluator):\n",
    "    \"\"\"LLM-as-a-judge correctness evaluator (version-safe).\"\"\"\n",
    "\n",
    "    def evaluate_run(self, run, example, **kwargs):\n",
    "        ref = example.outputs.get(\"answer\", \"\").strip()\n",
    "\n",
    "        pred = (\n",
    "            run.outputs.get(\"answer\")\n",
    "            or run.outputs.get(\"output_text\")\n",
    "            or run.outputs.get(\"result\")\n",
    "            or \"\"\n",
    "        ).strip()\n",
    "\n",
    "        question = example.inputs.get(\"question\", \"\")\n",
    "\n",
    "        if not ref or not pred:\n",
    "            return {\n",
    "                \"key\": \"correctness\",\n",
    "                \"score\": 0.0,\n",
    "                \"commentary\": \"Missing reference or prediction\",\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            score, reason = self.llm_as_a_judge(ref, pred, question)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"key\": \"correctness\",\n",
    "                \"score\": 0.0,\n",
    "                \"commentary\": f\"Evaluator error: {e}\",\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": score,\n",
    "            \"commentary\": reason,\n",
    "        }\n",
    "\n",
    "    def llm_as_a_judge(self, reference: str, prediction: str, question: str = \"\"):\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a semantic correctness evaluator.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "Question: {question}\n",
    "Reference answer: {reference}\n",
    "Model prediction: {prediction}\n",
    "\n",
    "Return JSON only:\n",
    "{{\"score\": <number between 0 and 1>, \"reason\": \"<short explanation>\"}}\n",
    "\"\"\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        data = json.loads(content)\n",
    "\n",
    "        score = float(data[\"score\"])\n",
    "        reason = data[\"reason\"]\n",
    "\n",
    "        return max(0.0, min(1.0, score)), reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711b79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'langsmith_eval_test-a6a662cf' at:\n",
      "https://smith.langchain.com/o/79133fd6-316c-4b99-b886-38847131d1e1/datasets/9a93e5d3-cdc8-4a90-aef2-9de7206c5a93/compare?selectedSessions=8e89748e-f376-46be-bf48-c5e3e5f60761\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:19,  4.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "simple_correctness = SimpleCorrectness()\n",
    "\n",
    "results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[simple_correctness],\n",
    "    experiment_prefix=\"langsmith_eval_test\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a66ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is Mark's current work status?\n",
      "Expected: Mark is on a leave of absence until next Monday.\n",
      "Predicted: Mark is currently on a leave of absence until next Monday.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EvaluationResult' object has no attribute 'commentary'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPredicted: (no output found)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m eval_results:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     16\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluator: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExplanation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommentary\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'EvaluationResult' object has no attribute 'commentary'"
     ]
    }
   ],
   "source": [
    "for r in results:   # each r is a dict\n",
    "    example = r[\"example\"]\n",
    "    eval_results = r[\"evaluation_results\"][\"results\"]\n",
    "\n",
    "    print(f\"\\n Question: {example.inputs['question']}\")\n",
    "    print(f\"Expected: {example.outputs['answer']}\")\n",
    "\n",
    "    # Extract model output\n",
    "    run = r[\"run\"]\n",
    "    if hasattr(run, \"outputs\") and \"answer\" in run.outputs:\n",
    "        print(f\"Predicted: {run.outputs['answer']}\")\n",
    "    else:\n",
    "        print(\"Predicted: (no output found)\")\n",
    "\n",
    "    # Print evaluator results\n",
    "    for e in eval_results:\n",
    "        print(f\"Evaluator: {e.key}, Score: {e.score}, Explanation: {getattr(e, 'explanation', None)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a93ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
